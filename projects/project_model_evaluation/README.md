<div align="center">

# âœ… Model Evaluation Framework Project

### *Comprehensive Assessment Toolkit for ML Models*

![Status](https://img.shields.io/badge/Status-Complete-brightgreen?style=flat-square)
![Type](https://img.shields.io/badge/Type-Evaluation-blue?style=flat-square)
![Notebooks](https://img.shields.io/badge/Notebooks-4-blue?style=flat-square)
![Level](https://img.shields.io/badge/Level-Intermediate-yellow?style=flat-square)

**Classification & regression metrics, cross-validation, profit curves, and calibration**

[ğŸ¯ Goal](#-project-goal) â€¢ [ğŸ“š Notebooks](#-notebooks) â€¢ [ğŸ“Š Metrics](#-metrics-covered) â€¢ [ğŸš€ Run It](#-quick-start)

</div>

---

## ğŸ¯ Project Goal

> **"Accuracy is not enough. Choose the right metric and validate properly."**

**Build a production-ready evaluation module** with comprehensive metrics, cross-validation strategies, and business-aligned assessment for all future projects.

---

## ğŸ“š Notebooks

**4 comprehensive notebooks covering the complete evaluation workflow**

<table>
<tr>
<td width="50%">

### ğŸ¯ Notebook 1: Baseline & Accuracy Trap
**`01_baseline_and_accuracy_trap.ipynb`**

**The Accuracy Fallacy:**
- âœ… **Dummy Classifier:** Baseline setup
- âœ… **Imbalanced Data:** 95% accuracy trap
- âœ… **Confusion Matrix:** True understanding
- âœ… **Class Distribution:** Impact demonstration

**Key Example:**
```
Fraud Detection (99% non-fraud):
- Predict all "no fraud" â†’ 99% accuracy!
- But catches 0% of actual fraud âŒ
```

**Lesson:** Accuracy lies with imbalance

---

### ğŸ“Š Notebook 2: Advanced Metrics
**`02_advanced_metrics.ipynb`**

**Beyond Accuracy:**
- âœ… **Precision:** Of predicted positives, how many correct?
- âœ… **Recall:** Of actual positives, how many caught?
- âœ… **F1-Score:** Harmonic mean balance
- âœ… **ROC-AUC:** Threshold-independent
- âœ… **PR-AUC:** Better for imbalance
- âœ… **Matthews** Correlation:** Balanced metric

**Comparison:**
- Metric selection guide
- imbalance-class handling
- Multi-class extensions

**Output:** Metric decision flowchart

</td>
<td width="50%">

### ğŸ’° Notebook 3: Profit Curves
**`03_profit_curves.ipynb`**

**Business-Aligned Metrics:**
- âœ… **Cost-Benefit Matrix:**
  - True Positive value
  - False Positive cost
  - False Negative cost
- âœ… **Profit Curves:** $ vs threshold
- âœ… **Optimal Threshold:** Max profit point
- âœ… **ROI Calculation:** Expected value

**Real Example:**
```
Marketing Campaign:
- True Positive: +$50 (conversion)
- False Positive: -$5 (wasted ad)
- Optimal threshold: 0.35 (not 0.5!)
- Profit increase: +$45K
```

**Output:** Business-optimized thresholds

---

### ğŸ“ Notebook 4: Calibration
**`04_calibration.ipynb`**

**Probability Reliability:**
- âœ… **Calibration Plots:** Predicted vs actual
- âœ… **Brier Score:** Probabilistic accuracy
- âœ… **Calibration Methods:**
  - Platt scaling (logistic)
  - Isotonic regression
- âœ… **Model Comparison:** Pre/post calibration

**Why It Matters:**
- "80% probability" should mean 80%!
- Critical for decision-making
- Affects cost-sensitive predictions

**Output:** Calibrated probability models

</td>
</tr>
</table>

---

## ğŸ“Š Metrics Covered

<details>
<summary><strong>ğŸ“Š Classification Metrics (8 Metrics)</strong></summary>

| Metric | Formula | When to Use | Range |
|--------|---------|-------------|-------|
| **Accuracy** | (TP+TN)/Total | Balanced classes only | [0,1] |
| **Precision** | TP/(TP+FP) | Minimize false alarms | [0,1] |
| **Recall** | TP/(TP+FN) | Catch all positives | [0,1] |
| **F1-Score** | 2Ã—PÃ—R/(P+R) | Balance P and R | [0,1] |
| **ROC-AUC** | Area under curve | Threshold-free | [0,1] |
| **PR-AUC** | Precision-Recall area | Imbalanced data | [0,1] |
| **MCC** | Correlation coefficient | Balanced, any class ratio | [-1,1] |
| **Cohen's Kappa** | Agreement vs chance | Multi-class | [-1,1] |

</details>

<details>
<summary><strong>ğŸ“ Regression Metrics (6 Metrics)</strong></summary>

| Metric | Formula | Interpretation | Best Value |
|--------|---------|----------------|------------|
| **MAE** | Î£\|y-Å·\|/n | Average error (same units) | 0 |
| **MSE** | Î£(y-Å·)Â²/n | Penalizes large errors | 0 |
| **RMSE** | âˆšMSE | Error in original units | 0 |
| **RÂ²** | 1-SS_res/SS_tot | % variance explained | 1 |
| **Adj RÂ²** | Penalized RÂ² | Accounts for features | 1 |
| **MAPE** | Î£\|y-Å·\|/y | Percentage error | 0 |

</details>

<details>
<summary><strong>ğŸ”„ Cross-Validation Methods</strong></summary>

**Standard:**
- K-Fold (k=5 or 10)
- Stratified K-Fold (preserves class distribution)
- Leave-One-Out (expensive but thorough)

**Specialized:**
- Time Series CV (respects temporal order)
- Group K-Fold (keeps groups together)
- Repeated K-Fold (multiple runs)

**Code Example:**
```python
from sklearn.model_selection import cross_val_score, StratifiedKFold

# For imbalanced classification
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=cv, scoring='f1')

print(f"F1: {scores.mean():.3f} Â± {scores.std():.3f}")
```

</details>

---

## ğŸ’¡ Key Learnings

<details>
<summary><strong>The Accuracy Trap (Real Example)</strong></summary>

**Credit Card Fraud Detection:**

```
Dataset: 10,000 transactions, 100 fraudulent (1%)

Model 1: Predicts all "legitimate"
- Accuracy: 99% âœ…
- Recall: 0% âŒ (catches NO fraud!)

Model 2: Tuned for recall
- Accuracy: 94% 
- Recall: 85% âœ… (catches 85 of 100 frauds)

Winner: Model 2 (despite lower accuracy!)
```

**Lesson:** Accuracy is misleading with imbalance

</details>

<details>
<summary><strong>Optimal Threshold â‰  0.5</strong></summary>

**Marketing Campaign Example:**

```
Cost-Benefit:
- True Positive: +$100 (customer acquired)
- False Positive: -$10 (wasted marketing)

Threshold Analysis:
- 0.5 (default): $50K profit
- 0.3 (optimized): $78K profit (+56%!)
- 0.7 (too high): $22K profit

Optimal: 0.32 (maximizes expected profit)
```

**Lesson:** Business metrics drive threshold selection

</details>

<details>
<summary><strong>Calibration Matters</strong></summary>

**Medical Diagnosis Example:**

**Uncalibrated Model:**
```
Says "70% probability of disease"
Actual rate: 40% (overconfident!)
```

**After Platt Scaling:**
```
Says "70% probability"
Actual rate: 68% âœ… (well-calibrated)
```

**Impact:** Doctors can trust probabilities for treatment decisions

</details>

---

## ğŸš€ Quick Start

### Installation

```bash
# Navigate to project
cd projects/project_model_evaluation

# Install dependencies
pip install -r requirements.txt
```

### Run Notebooks

```bash
# Launch Jupyter
jupyter notebook notebooks/

# Execute in order:
# 1. 01_baseline_and_accuracy_trap.ipynb
# 2. 02_advanced_metrics.ipynb
# 3. 03_profit_curves.ipynb
# 4. 04_calibration.ipynb
```

---

## ğŸ¯ Metric Selection Guide

**Quick Decision Tree:**

```
Your Problem:
â”‚
â”œâ”€ Imbalanced Classes?
â”‚  â”œâ”€ Yes â†’ Use F1, PR-AUC, MCC (NOT accuracy)
â”‚  â””â”€ No â†’ Accuracy OK
â”‚
â”œâ”€ Cost-Sensitive?
â”‚  â”œâ”€ Yes â†’ Use Profit Curves (custom threshold)
â”‚  â””â”€ No â†’ Use standard metrics
â”‚
â”œâ”€ Need Probabilities?
â”‚  â”œâ”€ Yes â†’ Calibrate model (Platt/Isotonic)
â”‚  â””â”€ No â†’ Hard predictions OK
â”‚
â””â”€ Multi-Class?
   â”œâ”€ Yes â†’ Macro/Micro/Weighted averaging
   â””â”€ No â†’ Binary metrics
```

---

## ğŸ’¼ Business Value

**Production-Ready Evaluation:**

| Scenario | Metric | Why |
|----------|--------|-----|
| **Fraud Detection** | Recall, F2 | Minimize false negatives|
| **Email Spam** | Precision, F0.5 | Minimize false positives |
| **Credit Approval** | Profit Curve | Maximize revenue |
| **Medical Diagnosis** | Calibrated probabilities | Trust thresholds |
| **Recommendation** | PR-AUC | Imbalanced (few clicks) |

---

## ğŸ“ What You'll Master

<table>
<tr>
<td width="50%">

### ğŸ“Š Metrics Mastery
- âœ… 8 classification metrics
- âœ… 6 regression metrics
- âœ… When to use each
- âœ… Interpretation pitfalls
- âœ… Multi-class extensions

</td>
<td width="50%">

### ğŸ”¬ Advanced Techniques
- âœ… Cross-validation strategies
- âœ… Profit curve optimization
- âœ… Model calibration
- âœ… Threshold tuning
- âœ… Business-aligned evaluation

</td>
</tr>
</table>

---

## ğŸ“ Project Structure

```
project_model_evaluation/
â”œâ”€â”€ ğŸ“Š notebooks/          # 4 comprehensive guides
â”‚   â”œâ”€â”€ 01_baseline_and_accuracy_trap.ipynb
â”‚   â”œâ”€â”€ 02_advanced_metrics.ipynb
â”‚   â”œâ”€â”€ 03_profit_curves.ipynb
â”‚   â””â”€â”€ 04_calibration.ipynb
â”‚
â”œâ”€â”€ ğŸ”§ src/                # Reusable evaluation functions
â”‚   â”œâ”€â”€ metrics.py
â”‚   â”œâ”€â”€ calibration.py
â”‚   â”œâ”€â”€ profit_curves.py
â”‚   â””â”€â”€ visualization.py
â”‚
â””â”€â”€ ğŸ“„ requirements.txt    # Dependencies
```

---

## ğŸ† Key Takeaways

> **"The right metric can change everything. A 0.32 threshold (not 0.5) increased profit by $28K in the marketing example."**

**For Data Scientists:**
- âœ… Accuracy fails with imbalance
- âœ… F1, PR-AUC better for real problems
- âœ… Business metrics beat statistical metrics
- âœ… Calibration enables trust in probabilities

---

## ğŸ”— Related Resources

**Continue Learning:**
- ğŸ“š [Evaluation Module](../../learning/05_evaluation/) - Theory & concepts
- ğŸ“ [Telco Churn](../project_telco_churn/) - Metrics in action ($3.9M)
- âš™ï¸ [Feature Engineering](../project_feature_engineering/) - Pipeline integration

---

<div align="center">

**Measure Right, Build Right** âœ…

*4 notebooks â€¢ 14+ metrics â€¢ Business-aligned evaluation*

[â¬…ï¸ Feature Engineering](../project_feature_engineering/) â€¢ [ğŸ  Home](../../README.md) â€¢ [â¡ï¸ Text EDA](../project_text_eda/)

</div>
