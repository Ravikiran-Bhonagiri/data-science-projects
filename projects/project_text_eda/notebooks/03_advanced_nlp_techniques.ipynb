{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 3. Advanced NLP Techniques\n",
                "\n",
                "**Project:** Text EDA (20 Newsgroups)  \n",
                "**Goal:** Apply advanced NLP techniques including Named Entity Recognition, POS tagging, text classification, and advanced topic modeling.\n",
                "\n",
                "---\n",
                "\n",
                "## 1. Imports and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "from sklearn.datasets import fetch_20newsgroups\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
                "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
                "from sklearn.model_selection import train_test_split, cross_val_score\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "import os\n",
                "\n",
                "# Download required NLTK data\n",
                "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
                "nltk.download('maxent_ne_chunker', quiet=True)\n",
                "nltk.download('words', quiet=True)\n",
                "\n",
                "# Try importing spaCy (show error if not installed)\n",
                "try:\n",
                "    import spacy\n",
                "    nlp = spacy.load('en_core_web_sm')\n",
                "    SPACY_AVAILABLE = True\n",
                "except:\n",
                "    print(\"spaCy not available. Install with: pip install spacy && python -m spacy download en_core_web_sm\")\n",
                "    SPACY_AVAILABLE = False\n",
                "\n",
                "# Try importing TextBlob\n",
                "try:\n",
                "    from textblob import TextBlob\n",
                "    TEXTBLOB_AVAILABLE = True\n",
                "except:\n",
                "    print(\"TextBlob not available. Install with: pip install textblob\")\n",
                "    TEXTBLOB_AVAILABLE = False\n",
                "\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Dataset\n",
                "We'll continue using the 20 Newsgroups dataset from previous notebooks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "categories = ['sci.space', 'comp.graphics', 'talk.politics.mideast', 'rec.sport.hockey']\n",
                "data_home = '../../data/raw/scikit_learn_data'\n",
                "if not os.path.exists(data_home):\n",
                "    os.makedirs(data_home)\n",
                "\n",
                "newsgroups = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), \n",
                "                               categories=categories, data_home=data_home)\n",
                "\n",
                "df = pd.DataFrame({'text': newsgroups.data, 'target': newsgroups.target})\n",
                "df['category'] = df['target'].map(lambda x: newsgroups.target_names[x])\n",
                "\n",
                "print(f\"Dataset Shape: {df.shape}\")\n",
                "print(f\"Categories: {newsgroups.target_names}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Named Entity Recognition (NER)\n",
                "Extract and analyze named entities (persons, organizations, locations) from the text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if SPACY_AVAILABLE:\n",
                "    # Sample subset for NER (processing all texts is slow)\n",
                "    sample_texts = df['text'].sample(100, random_state=42)\n",
                "    \n",
                "    entities = {'PERSON': [], 'ORG': [], 'GPE': []}\n",
                "    \n",
                "    for text in sample_texts:\n",
                "        doc = nlp(text[:1000])  # Limit to first 1000 chars for speed\n",
                "        for ent in doc.ents:\n",
                "            if ent.label_ in entities:\n",
                "                entities[ent.label_].append(ent.text)\n",
                "    \n",
                "    # Visualize top entities\n",
                "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "    \n",
                "    for idx, (entity_type, entity_list) in enumerate(entities.items()):\n",
                "        if entity_list:\n",
                "            entity_counts = pd.Series(entity_list).value_counts().head(10)\n",
                "            axes[idx].barh(entity_counts.index, entity_counts.values)\n",
                "            axes[idx].set_title(f'Top {entity_type} Entities')\n",
                "            axes[idx].set_xlabel('Frequency')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    print(f\"Total Persons: {len(entities['PERSON'])}\")\n",
                "    print(f\"Total Organizations: {len(entities['ORG'])}\")\n",
                "    print(f\"Total Locations: {len(entities['GPE'])}\")\n",
                "else:\n",
                "    print(\"spaCy not available for NER analysis\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Part-of-Speech (POS) Tagging\n",
                "Analyze the grammatical structure of the text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if SPACY_AVAILABLE:\n",
                "    # Analyze POS distribution\n",
                "    sample_text = df['text'].iloc[0]\n",
                "    doc = nlp(sample_text[:500])\n",
                "    \n",
                "    pos_tags = [token.pos_ for token in doc]\n",
                "    pos_counts = pd.Series(pos_tags).value_counts()\n",
                "    \n",
                "    plt.figure(figsize=(10, 6))\n",
                "    sns.barplot(x=pos_counts.values, y=pos_counts.index, palette='viridis')\n",
                "    plt.title('Part-of-Speech Distribution (Sample Text)')\n",
                "    plt.xlabel('Count')\n",
                "    plt.ylabel('POS Tag')\n",
                "    plt.show()\n",
                "    \n",
                "    # Extract noun phrases\n",
                "    noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
                "    print(f\"\\nSample Noun Phrases: {noun_phrases[:10]}\")\n",
                "else:\n",
                "    print(\"spaCy not available for POS tagging\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Text Classification\n",
                "Build and compare classification models to predict document categories."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare data for classification\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    df['text'], df['target'], test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "# TF-IDF vectorization\n",
                "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
                "X_train_tfidf = tfidf.fit_transform(X_train)\n",
                "X_test_tfidf = tfidf.transform(X_test)\n",
                "\n",
                "print(f\"Training set size: {X_train_tfidf.shape}\")\n",
                "print(f\"Test set size: {X_test_tfidf.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Logistic Regression\n",
                "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
                "lr_model.fit(X_train_tfidf, y_train)\n",
                "lr_pred = lr_model.predict(X_test_tfidf)\n",
                "\n",
                "print(\"Logistic Regression Results:\")\n",
                "print(classification_report(y_test, lr_pred, target_names=newsgroups.target_names))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Naive Bayes\n",
                "nb_model = MultinomialNB()\n",
                "nb_model.fit(X_train_tfidf, y_train)\n",
                "nb_pred = nb_model.predict(X_test_tfidf)\n",
                "\n",
                "print(\"Naive Bayes Results:\")\n",
                "print(classification_report(y_test, nb_pred, target_names=newsgroups.target_names))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix Comparison\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "for idx, (pred, title) in enumerate([(lr_pred, 'Logistic Regression'), (nb_pred, 'Naive Bayes')]):\n",
                "    cm = confusion_matrix(y_test, pred)\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
                "                xticklabels=newsgroups.target_names, yticklabels=newsgroups.target_names)\n",
                "    axes[idx].set_title(f'{title} Confusion Matrix')\n",
                "    axes[idx].set_ylabel('True Label')\n",
                "    axes[idx].set_xlabel('Predicted Label')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Advanced Topic Modeling\n",
                "Compare LDA vs NMF and evaluate with coherence scores."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare data for topic modeling\n",
                "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english', max_features=1000)\n",
                "doc_term_matrix = vectorizer.fit_transform(df['text'])\n",
                "\n",
                "n_topics = 4\n",
                "n_top_words = 10\n",
                "\n",
                "# LDA Model\n",
                "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
                "lda.fit(doc_term_matrix)\n",
                "\n",
                "# NMF Model\n",
                "nmf = NMF(n_components=n_topics, random_state=42)\n",
                "nmf.fit(doc_term_matrix)\n",
                "\n",
                "print(\"Models trained successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def display_topics(model, feature_names, n_top_words):\n",
                "    topics = []\n",
                "    for topic_idx, topic in enumerate(model.components_):\n",
                "        top_words = [feature_names[i] for i in topic.argsort()[-n_top_words:][::-1]]\n",
                "        topics.append(f\"Topic {topic_idx}: {' '.join(top_words)}\")\n",
                "    return topics\n",
                "\n",
                "feature_names = vectorizer.get_feature_names_out()\n",
                "\n",
                "print(\"\\nLDA Topics:\")\n",
                "lda_topics = display_topics(lda, feature_names, n_top_words)\n",
                "for topic in lda_topics:\n",
                "    print(topic)\n",
                "\n",
                "print(\"\\nNMF Topics:\")\n",
                "nmf_topics = display_topics(nmf, feature_names, n_top_words)\n",
                "for topic in nmf_topics:\n",
                "    print(topic)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate models with perplexity (for LDA)\n",
                "lda_perplexity = lda.perplexity(doc_term_matrix)\n",
                "print(f\"\\nLDA Perplexity: {lda_perplexity:.2f}\")\n",
                "print(\"(Lower perplexity indicates better performance)\")\n",
                "\n",
                "# Calculate reconstruction error for NMF\n",
                "W = nmf.transform(doc_term_matrix)\n",
                "H = nmf.components_\n",
                "reconstruction_error = np.linalg.norm(doc_term_matrix.toarray() - W @ H, 'fro')\n",
                "print(f\"NMF Reconstruction Error: {reconstruction_error:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Multiple Sentiment Analysis Methods\n",
                "Compare VADER and TextBlob sentiment analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
                "\n",
                "# VADER sentiment\n",
                "sia = SentimentIntensityAnalyzer()\n",
                "df['vader_sentiment'] = df['text'].apply(lambda x: sia.polarity_scores(x[:500])['compound'])\n",
                "\n",
                "# TextBlob sentiment\n",
                "if TEXTBLOB_AVAILABLE:\n",
                "    df['textblob_sentiment'] = df['text'].apply(lambda x: TextBlob(x[:500]).sentiment.polarity)\n",
                "    \n",
                "    # Compare methods\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    \n",
                "    for idx, col in enumerate(['vader_sentiment', 'textblob_sentiment']):\n",
                "        sns.histplot(data=df, x=col, hue='category', kde=True, bins=30, ax=axes[idx])\n",
                "        axes[idx].set_title(f'{col.replace(\"_\", \" \").title()} Distribution')\n",
                "        axes[idx].set_xlabel('Sentiment Score')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # Correlation between methods\n",
                "    correlation = df[['vader_sentiment', 'textblob_sentiment']].corr().iloc[0, 1]\n",
                "    print(f\"\\nCorrelation between VADER and TextBlob: {correlation:.3f}\")\n",
                "    \n",
                "    # Mean sentiment by category\n",
                "    print(\"\\nMean Sentiment by Category:\")\n",
                "    print(df.groupby('category')[['vader_sentiment', 'textblob_sentiment']].mean())\n",
                "else:\n",
                "    # Only VADER\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    sns.histplot(data=df, x='vader_sentiment', hue='category', kde=True, bins=30)\n",
                "    plt.title('VADER Sentiment Distribution by Category')\n",
                "    plt.xlabel('Sentiment Score')\n",
                "    plt.show()\n",
                "    \n",
                "    print(\"\\nMean VADER Sentiment by Category:\")\n",
                "    print(df.groupby('category')['vader_sentiment'].mean())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}