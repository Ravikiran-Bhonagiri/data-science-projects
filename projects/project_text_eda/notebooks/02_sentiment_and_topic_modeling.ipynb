{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2. Sentiment Analysis and Topic Modeling\n",
                "\n",
                "**Project:** Text EDA\n",
                "**Goal:** Beyond counting words - understanding emotion (Sentiment) and hidden themes (Topic Modeling).\n",
                "\n",
                "---\n",
                "\n",
                "## 1. Imports and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import nltk\n",
                "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
                "from sklearn.decomposition import LatentDirichletAllocation\n",
                "from sklearn.datasets import fetch_20newsgroups\n",
                "import re\n",
                "import os\n",
                "\n",
                "nltk.download('vader_lexicon')\n",
                "\n",
                "sns.set_style('whitegrid')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load and Preprocess Data (Re-run)\n",
                "We reload the dataset to keep notebooks independent."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "categories = ['sci.space', 'comp.graphics', 'talk.politics.mideast', 'rec.sport.hockey']\n",
                "data_home = '../../data/raw/scikit_learn_data'\n",
                "if not os.path.exists(data_home):\n",
                "    os.makedirs(data_home)\n",
                "\n",
                "newsgroups = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), \n",
                "                               categories=categories, data_home=data_home)\n",
                "df = pd.DataFrame({'text': newsgroups.data, 'target': newsgroups.target})\n",
                "df['category'] = df['target'].map(lambda x: newsgroups.target_names[x])\n",
                "\n",
                "# Minimal cleaning for VADER (punctuation can help sentiment)\n",
                "# But for LDA we need clean tokens\n",
                "def clean_for_lda(text):\n",
                "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
                "    return text\n",
                "\n",
                "df['clean_text'] = df['text'].apply(clean_for_lda)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Sentiment Analysis (VADER)\n",
                "VADER (Valence Aware Dictionary and sEntiment Reasoner) is excellent for social media and general text. It gives a 'Compound' score from -1 (Negative) to +1 (Positive)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sia = SentimentIntensityAnalyzer()\n",
                "\n",
                "# Apply VADER to first 500 characters of each text (speed optimization)\n",
                "df['sentiment_score'] = df['text'].apply(lambda x: sia.polarity_scores(x[:500])['compound'])\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.histplot(data=df, x='sentiment_score', hue='category', kde=True, bins=30, palette='tab10')\n",
                "plt.title('Sentiment Score Distribution by Category')\n",
                "plt.xlabel('Compound Sentiment Score (-1 to 1)')\n",
                "plt.show()\n",
                "\n",
                "print(\"Mean Sentiment by Category:\")\n",
                "print(df.groupby('category')['sentiment_score'].mean().sort_values(ascending=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Topic Modeling (LDA)\n",
                "Latent Dirichlet Allocation finds groups of words that appear together 'topics'.\n",
                "\n",
                "We use **TF-IDF** vectorization to downweight common words."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
                "tfidf = tfidf_vectorizer.fit_transform(df['clean_text'])\n",
                "\n",
                "# Fit LDA with 4 topics (since we have 4 categories)\n",
                "lda = LatentDirichletAllocation(n_components=4, random_state=42)\n",
                "lda.fit(tfidf)\n",
                "\n",
                "def print_top_words(model, feature_names, n_top_words):\n",
                "    for topic_idx, topic in enumerate(model.components_):\n",
                "        message = f\"Topic #{topic_idx}: \"\n",
                "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
                "        print(message)\n",
                "    print()\n",
                "\n",
                "print(\"Top words per Laten Topic:\")\n",
                "print_top_words(lda, tfidf_vectorizer.get_feature_names_out(), 10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Visualizing Topics (t-SNE)\n",
                "We project the high-dimensional TF-IDF vectors to 2D to see if the categories form distinct clusters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.manifold import TSNE\n",
                "\n",
                "# Sample for speed\n",
                "sample_idx = np.random.choice(tfidf.shape[0], 500, replace=False)\n",
                "X_sample = tfidf[sample_idx]\n",
                "y_sample = df['category'].iloc[sample_idx].values\n",
                "\n",
                "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')\n",
                "X_embedded = tsne.fit_transform(X_sample)\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.scatterplot(x=X_embedded[:,0], y=X_embedded[:,1], hue=y_sample, palette='viridis', s=60, alpha=0.8)\n",
                "plt.title('t-SNE Projection of Document Vectors')\n",
                "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}