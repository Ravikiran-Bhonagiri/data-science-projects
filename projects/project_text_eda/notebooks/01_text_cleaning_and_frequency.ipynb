{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 1. Text Cleaning and Frequency Analysis\n",
                "\n",
                "**Project:** Text EDA (20 Newsgroups)  \n",
                "**Goal:** Transform raw, messy text into clean tokens and analyze statistical properties (Zipf's Law, N-grams).\n",
                "\n",
                "---\n",
                "\n",
                "## 1. Imports and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import re\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "from wordcloud import WordCloud\n",
                "from sklearn.datasets import fetch_20newsgroups\n",
                "from collections import Counter\n",
                "import os\n",
                "\n",
                "# Ensure NLTK resources are available\n",
                "nltk.download('stopwords')\n",
                "nltk.download('wordnet')\n",
                "nltk.download('omw-1.4')\n",
                "\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Dataset\n",
                "We use the **20 Newsgroups** dataset. To make it challenging (and realistic), we strip headers, footers, and quotes, leaving only the raw message body."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "categories = ['sci.space', 'comp.graphics', 'talk.politics.mideast', 'rec.sport.hockey']\n",
                "# Download to local project folder to avoid cache corruption and permission issues\n",
                "data_home = '../../data/raw/scikit_learn_data'\n",
                "if not os.path.exists(data_home):\n",
                "    os.makedirs(data_home)\n",
                "\n",
                "newsgroups = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), \n",
                "                               categories=categories, data_home=data_home)\n",
                "\n",
                "df = pd.DataFrame({'text': newsgroups.data, 'target': newsgroups.target})\n",
                "df['category'] = df['target'].map(lambda x: newsgroups.target_names[x])\n",
                "\n",
                "print(f\"Dataset Shape: {df.shape}\")\n",
                "print(\"\\nSample Text:\")\n",
                "print(df['text'].iloc[0][:500])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Text Preprocessing Pipeline\n",
                "Raw text is noisy. We apply a standard cleaning pipeline:\n",
                "1.  **Lowercasing**: Uniformity.\n",
                "2.  **Regex Cleaning**: Remove special chars, numbers, and extra whitespace.\n",
                "3.  **Stopword Removal**: Remove common functional words (the, is, at).\n",
                "4.  **Lemmatization**: Convert words to base form (running -> run)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lemmatizer = WordNetLemmatizer()\n",
                "stop_words = set(stopwords.words('english'))\n",
                "\n",
                "def clean_text(text):\n",
                "    # 1. Lowercase\n",
                "    text = text.lower()\n",
                "    # 2. Remove special chars and numbers\n",
                "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
                "    # 3. Tokenize (simple split)\n",
                "    tokens = text.split()\n",
                "    # 4. Remove stopwords and lemmatize\n",
                "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
                "    \n",
                "    return \" \".join(tokens)\n",
                "\n",
                "df['clean_text'] = df['text'].apply(clean_text)\n",
                "\n",
                "print(\"Cleaning Complete. Sample Comparison:\")\n",
                "print(f\"Original: {df['text'].iloc[0][:100]}...\")\n",
                "print(f\"Clean:    {df['clean_text'].iloc[0][:100]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Word Frequency Analysis\n",
                "What are the most common terms in these domains?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_words = \" \".join(df['clean_text']).split()\n",
                "word_freq = Counter(all_words)\n",
                "\n",
                "common_words = pd.DataFrame(word_freq.most_common(20), columns=['Word', 'Count'])\n",
                "\n",
                "sns.barplot(x='Count', y='Word', data=common_words, palette='viridis')\n",
                "plt.title('Top 20 Most Frequent Words (Cleaned)')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Zipf's Law Verification\n",
                "Zipf's Law states that the frequency of any word is inversely proportional to its rank in the frequency table. On a log-log plot, this should appear as a straight line."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "counts = np.array(list(word_freq.values()))\n",
                "counts = -np.sort(-counts) # Descending sort\n",
                "ranks = np.arange(1, len(counts) + 1)\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.loglog(ranks, counts, marker=\".\", linestyle='none', color='purple')\n",
                "plt.title(\"Zipf's Law: Log-Log Plot of Word Frequency vs Rank\")\n",
                "plt.xlabel('Rank')\n",
                "plt.ylabel('Frequency')\n",
                "plt.grid(True, which=\"both\", ls=\"--\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Word Cloud\n",
                "Visualizing the corpus content."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='plasma').generate(\" \".join(all_words))\n",
                "\n",
                "plt.figure(figsize=(14, 7))\n",
                "plt.imshow(wordcloud, interpolation='bilinear')\n",
                "plt.axis('off')\n",
                "plt.title('Word Cloud of 20 Newsgroups (Subset)')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. N-Gram Analysis (Bi-grams)\n",
                "Single words miss context. Let's look at pairs of words (bi-grams)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "\n",
                "def plot_top_ngrams(text, n=2, top_k=15):\n",
                "    vec = CountVectorizer(ngram_range=(n, n), stop_words='english').fit(text)\n",
                "    bag_of_words = vec.transform(text)\n",
                "    sum_words = bag_of_words.sum(axis=0)\n",
                "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
                "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
                "    \n",
                "    df_ngram = pd.DataFrame(words_freq[:top_k], columns=['Ngram', 'Count'])\n",
                "    \n",
                "    plt.figure(figsize=(10, 6))\n",
                "    sns.barplot(x='Count', y='Ngram', data=df_ngram, palette='magma')\n",
                "    plt.title(f'Top {top_k} {n}-grams')\n",
                "    plt.show()\n",
                "\n",
                "plot_top_ngrams(df['clean_text'], n=2, top_k=15)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}