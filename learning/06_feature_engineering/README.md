<div align="center">

# âš™ï¸ Module 6: Feature Engineering

### *Transform Raw Data into Powerful Features*

![Status](https://img.shields.io/badge/Status-Complete-brightgreen?style=flat-square)
![Difficulty](https://img.shields.io/badge/Difficulty-Intermediate-yellow?style=flat-square)
![Topics](https://img.shields.io/badge/Topics-7-orange?style=flat-square)

**The secret sauce that turns good models into great ones**

[ğŸ”§ Encoding](#-encoding-techniques) â€¢ [ğŸ“ Scaling](#-scaling-methods) â€¢ [ğŸ¯ Selection](#-feature-selection)

</div>

---

## ğŸ’¡ Why Feature Engineering?

> **"Better features beat better algorithms."**

**Reality:**
- Raw data rarely works as-is
- Good features can 10Ã— model performance
- More important than hyperparameter tuning

---

## ğŸ”§ Encoding Techniques

**Handle categorical variables**

<table>
<tr>
<td width="50%">

### Standard Methods

**One-Hot Encoding**
```python
Gender: [Male, Female] â†’
Male_1, Male_0
Female_0, Female_1
```
âœ… Use when: Low cardinality (<10 categories)  
âŒ Avoid: High cardinality (explodes features)

**Label Encoding**
```python
Size: [S, M, L, XL] â†’ [0, 1, 2, 3]
```
âœ… Use: Ordinal categories  
âŒ Avoid: Nominal (implies order)

</td>
<td width="50%">

### Advanced Methods

**Target Encoding**
```python
City â†’ mean(target) per city
NYC: 0.65 (65% conversion)
LA:  0.52
```
âœ… Handles high cardinality  
âš ï¸ Watch for overfitting

**Frequency Encoding**
```python
City â†’ count / total
NYC: 0.35 (35% of data)
```
âœ… Simple, effective

**Binary Encoding**
- Converts to binary
- <features than one-hot

</td>
</tr>
</table>

---

## ğŸ“ Scaling Methods

**Normalize numerical features**

| Method | Formula | When to Use |
|--------|---------|-------------|
| **StandardScaler** | `(x - Î¼) / Ïƒ` | Most algorithms (SVM, KNN, Neural Nets) |
| **MinMaxScaler** | `(x - min) / (max - min)` | Bounded range [0,1] needed |
| **RobustScaler** | Uses median & IQR | Outliers present |
| **Normalizer** | Scale to unit norm | Text data (L1/L2 norm) |

**âš ï¸ Important:** Trees (Random Forest, XGBoost) DON'T need scaling!

---

## ğŸ¯ Feature Selection

**Choose the best features, remove noise**

<table>
<tr>
<td width="33%">

### Filter Methods
**Before modeling**

**Correlation**
- Remove highly correlated  
- Threshold: |r| > 0.9

**Mutual Information**
- Measures dependency
- Works with non-linear

**Chi-Square**
- For categorical
- Statistical test

</td>
<td width="33%">

### Wrapper Methods
**Use model feedback**

**Recursive Feature Elimination (RFE)**
```python
from sklearn.feature_selection import RFE
selector = RFE(model, n_features=10)
selector.fit(X, y)
```

**Forward/Backward Selection**
- Add/remove iteratively
- Greedy search

</td>
<td width="33%">

### Embedded Methods
**During training**

**L1 Regularization (Lasso)**
- Shrinks coefficients to 0
- Automatic selection

**Tree Feature Importance**
```python
importances = model.feature_importances_
```

**SelectFromModel**
- sklearn wrapper

</td>
</tr>
</table>

---

## ğŸ› ï¸ Feature Engineering Toolkit

### Numerical Transformations

```python
# Log Transform (right-skewed data)
df['price_log'] = np.log1p(df['price'])

# Polynomial Features
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

# Binning
df['age_group'] = pd.cut(df['age'], bins=[0, 18, 35, 50, 100])
```

### Feature Creation

```python
# Interactions
df['price_per_sqft'] = df['price'] / df['sqft']

# Aggregations
df['total_bedrooms'] = df['bed'] + df['bath']

# Time Features
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day_of_week'] = df['date'].dt.dayofweek
df['is_weekend'] = df['day_of_week'].isin([5, 6])
```

---

## ğŸ’¡ What You'll Master

<table>
<tr>
<td width="50%">

### ğŸ”§ Transformation Skills
- âœ… One-hot & label encoding
- âœ… Target & frequency encoding
- âœ… StandardScaler & alternatives
- âœ… Log & polynomial transforms
- âœ… Binning strategies

</td>
<td width="50%">

### ğŸ¯ Selection Skills
- âœ… Correlation analysis
- âœ… Mutual information
- âœ… Recursive elimination (RFE)
- âœ… L1 regularization
- âœ… Tree importance

</td>
</tr>
</table>

---

## ğŸš¨ Common Pitfalls

âŒ **Scaling before train/test split** â†’ Data leakage!  
âŒ **One-hot encoding high cardinality** â†’ Curse of dimensionality  
âŒ **Ignoring feature scaling** â†’ Poor SVM/KNN performance  
âŒ **No domain knowledge** â†’ Missing obvious features  
âŒ **Over-engineering** â†’ Complexity without gain  

---

## ğŸ¯ Complete Pipeline Example

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Define transformers
numeric_features = ['age', 'income', 'credit_score']
categorical_features = ['city', 'gender', 'occupation']

numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Full pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', XGBClassifier())
])

# Train
pipeline.fit(X_train, y_train)
```

---

<div align="center">

**Engineer Features, Engineer Success** âš™ï¸

*7 topics â€¢ Encoding + Scaling + Selection*

[â¬…ï¸ Model Evaluation](../05_evaluation/) â€¢ [ğŸ  Home](../../README.md) â€¢ [â¡ï¸ Unstructured Data](../07_unstructured_data/)

</div>
