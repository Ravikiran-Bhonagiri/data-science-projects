<div align="center">

# ğŸ¤– Module 3: Supervised Machine Learning

### *Teaching Machines to Predict*

![Status](https://img.shields.io/badge/Status-Complete-brightgreen?style=flat-square)
![Difficulty](https://img.shields.io/badge/Difficulty-Intermediate-yellow?style=flat-square)
![Guides](https://img.shields.io/badge/Guides-10-orange?style=flat-square)

**Master the algorithms powering 90% of industry ML jobs**

[ğŸ¯ Quick Guide](#-which-model) â€¢ [ğŸ“š Learning Path](#-learning-path) â€¢ [ğŸš€ Get Started](#-typical-workflow)

</div>

---

## ğŸ’¡ The Reality

> **"Master XGBoost and Random Forests â†’ Qualified for 90% of industry ML jobs"**

Supervised Learning = Teaching machines to map **Inputs (X)** â†’ **Outputs (y)**

---

## ğŸ“š Learning Path

<table>
<tr>
<td width="33%">

### ğŸ¯ Phase 1: Foundations
**Interview Essentials**

**[01. Linear Regression](./01_linear_regression.md)**
- Predict numbers
- Understand coefficients
- Baseline for everything

**[02. Logistic Regression](./02_logistic_regression.md)**
- Binary classification
- Probability outputs
- Interpretable results

</td>
<td width="33%">

### ğŸ† Phase 2: Production
**Kaggle Winners**

**[03. Decision Trees](./03_decision_trees.md)**
- Interpretable logic
- "20 Questions" approach

**[04. Random Forest](./04_ensemble_bagging.md)**
- Wisdom of crowds
- Robust, minimal tuning

**[05. XGBoost/LightGBM](./05_ensemble_boosting.md)**
- â­ **State-of-the-art**
- Highest accuracy
- Industry standard

</td>
<td width="33%">

### âš¡ Phase 3: Specialized
**Domain-Specific**

**[06. SVM](./06_support_vector_machines.md)**
- High-dimensional data
- Small datasets

**[07. KNN](./07_knn_algorithms.md)**
- Recommendations
- Anomaly detection

**[08. Naive Bayes](./08_naive_bayes.md)**
- Text classification
- NLP baseline

</td>
</tr>
</table>

### ğŸ“ Phase 4: Expert Techniques

**[09. Model Calibration](./09_model_calibration.md)** - Make probabilities trustworthy  
**[10. Hyperparameter Tuning](./10_hyperparameter_tuning.md)** - Automate optimization with Optuna

---

## ğŸ¯ Which Model?

**Quick decision guide:**

| Your Situation | Best Model | Why? |
|----------------|------------|------|
| ğŸ“Š **Tabular data** (Excel-like) | **XGBoost / LightGBM** | State-of-the-art accuracy |
| â±ï¸ **Need it working in 5 mins** | **Random Forest** | No tuning, handles messy data |
| ğŸ“ **Text data** (NLP) | **Naive Bayes / SVM** | Handles sparse high-dim data |
| ğŸ“– **Explanation critical** | **Linear / Logistic / Tree** | Show logic to stakeholders |
| âš¡ **Ultra-low latency** (<1ms) | **Logistic Regression** | Just a dot product |

---

## ğŸ› ï¸ Typical Workflow

```python
# 1. Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 2. Baseline (Always start simple!)
baseline = DummyClassifier(strategy="most_frequent")
baseline.fit(X_train, y_train)  # e.g., 70% accuracy

# 3. Real Model
model = XGBClassifier()
model.fit(X_train, y_train)  # e.g., 85% accuracy

# 4. Tune Hyperparameters
study = optuna.create_study()
study.optimize(objective_function)  # e.g., 88% accuracy

# 5. Evaluate
print(classification_report(y_test, predictions))
```

---

## ğŸ¯ What You'll Master

<table>
<tr>
<td width="50%">

### ğŸ“Š Core Algorithms
- âœ… Linear & Logistic Regression
- âœ… Decision Trees
- âœ… Random Forests
- âœ… XGBoost & LightGBM
- âœ… SVM, KNN, Naive Bayes

</td>
<td width="50%">

### ğŸš€ Advanced Skills
- âœ… Train/test splitting
- âœ… Cross-validation
- âœ… Hyperparameter tuning
- âœ… Model calibration
- âœ… Ensemble methods
- âœ… Production deployment

</td>
</tr>
</table>

---

<div align="center">

**Master Prediction, Master Data Science** ğŸ¤–

[â¬…ï¸ Statistics](../02_statistics/) â€¢ [ğŸ  Home](../../README.md) â€¢ [â¡ï¸ Unsupervised ML](../04_unsupervised_ml/)

</div>
