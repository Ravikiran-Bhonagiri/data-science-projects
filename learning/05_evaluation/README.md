<div align="center">

# âš–ï¸ Module 5: Model Evaluation & Metrics

### *Measuring What Matters*

![Status](https://img.shields.io/badge/Status-Complete-brightgreen?style=flat-square)
![Difficulty](https://img.shields.io/badge/Difficulty-Intermediate-yellow?style=flat-square)
![Topics](https://img.shields.io/badge/Topics-9-orange?style=flat-square)

**Master the art of assessing model performance beyond accuracy**

[ğŸ“Š Classification](#-classification-metrics) â€¢ [ğŸ“ Regression](#-regression-metrics) â€¢ [ğŸ”„ Validation](#-cross-validation)

</div>

---

## ğŸ’¡ Why Evaluation Matters

> **"Accuracy is not enough. Choose the right metric for your problem."**

**Reality check:**
- 95% accuracy on fraud detection = USELESS if you miss all frauds
- Lower RMSE doesn't mean better business value
- Cross-validation prevents overfitting disasters

---

## ğŸ“Š Classification Metrics

**For binary and multi-class problems**

<table>
<tr>
<td width="50%">

### Core Metrics

**Accuracy**
```
(TP + TN) / Total
```
âš ï¸ Misleading with imbalanced classes

**Precision**
```
TP / (TP + FP)
```
âœ… "Of predicted positives, how many correct?"

**Recall (Sensitivity)**
```
TP / (TP + FN)
```
âœ… "Of actual positives, how many caught?"

**F1-Score**  
```
2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```
âœ… Harmonic mean, balances both

</td>
<td width="50%">

### Advanced Metrics

**ROC-AUC**
- Area under ROC curve
- Threshold-independent
- Good for class imbalance

**Precision-Recall AUC**
- Better than ROC for severe imbalance
- Focuses on positive class

**Cohen's Kappa**
- Accounts for chance agreement
- For multi-class

**Matthews Correlation Coefficient**
- Balanced even with imbalance
- Range: -1 to +1

</td>
</tr>
</table>

---

## ğŸ“ Regression Metrics

**For continuous predictions**

| Metric | Formula | When to Use |
|--------|---------|-------------|
| **MAE** | `Î£\|y - Å·\| / n` | Same unit as target, interpretable |
| **MSE** | `Î£(y - Å·)Â² / n` | Penalizes large errors more |
| **RMSE** | `âˆšMSE` | Same units, more sensitive |
| **RÂ²** | `1 - SS_res/SS_tot` | % variance explained (0-1) |
| **Adjusted RÂ²** | Penalized RÂ² | Accounts for # features |
| **MAPE** | `Î£\|y - Å·\|/y Ã— 100` | Percentage error |

---

## ğŸ”„ Cross-Validation

**Ensure generalization**

<table>
<tr>
<td width="50%">

### Standard Methods

**K-Fold CV**
```python
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5)
print(f"Mean: {scores.mean():.3f} Â±{scores.std():.3f}")
```

**Stratified K-Fold**
- Preserves class distribution
- Critical for imbalanced data

**Leave-One-Out (LOO)**
- K = n (each sample is test once)
- Expensive but thorough

</td>
<td width="50%">

### Specialized Methods

**Time Series CV**
- No random splits
- Respects temporal order
- Expanding/sliding window

**Group K-Fold**
- Keep groups together
- Patient data, documents

**Repeated K-Fold**
- Run K-fold multiple times
- More robust estimates

</td>
</tr>
</table>

---

## ğŸ¯ Metric Selection Guide

**Choose based on your problem:**

### Imbalanced Classification (e.g., Fraud Detection)
```
âŒ Accuracy (misleading)
âœ… Precision-Recall AUC
âœ… F1-Score
âœ… Matthews Correlation Coefficient
```

### Medical Diagnosis (minimize false negatives)
```
âœ… Recall (catch all positives)  
âœ… F2-Score (weights recall 2Ã—)
âš ï¸ Precision (less critical here)
```

### Spam Detection (minimize false positives)
```
âœ… Precision (avoid blocking real email)
âœ… F0.5-Score (weights precision 2Ã—)
âš ï¸ Recall (some spam OK to miss)
```

### Regression (House Prices)
```
âœ… RMSE (penalize big errors)
âœ… MAPE (% errors interpretable)
âŒ MSE (units squared, hard to interpret)
```

---

## ğŸ› ï¸ Complete Evaluation Pipeline

```python
from sklearn.model_selection import cross_validate
from sklearn.metrics import make_scorer, f1_score, roc_auc_score

# Define multiple metrics
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1': 'f1',
    'roc_auc': 'roc_auc'
}

# Cross-validate with all metrics
results = cross_validate(
    model, X, y,
    cv=5,
    scoring=scoring,
    return_train_score=True
)

# Analyze
for metric in scoring:
    train_mean = results[f'train_{metric}'].mean()
    test_mean = results[f'test_{metric}'].mean()
    gap = train_mean - test_mean
    
    print(f"{metric}: Train={train_mean:.3f}, Test={test_mean:.3f}, Gap={gap:.3f}")
    if gap > 0.1:
        print("âš ï¸ Overfitting detected!")
```

---

## ğŸ’¡ What You'll Master

<table>
<tr>
<td width="50%">

### ğŸ“Š Classification
- âœ… Confusion matrix interpretation
- âœ… Precision vs Recall tradeoff
- âœ… ROC and PR curves
- âœ… Multi-class metrics
- âœ… Imbalanced data handling

</td>
<td width="50%">

### ğŸ“ Regression
- âœ… MAE, MSE, RMSE differences
- âœ… RÂ² interpretation
- âœ… Residual analysis
- âœ… Custom metrics
- âœ… Business-aligned metrics

</td>
</tr>
</table>

---

## ğŸš¨ Common Pitfalls

**Avoid these mistakes:**

âŒ **Using accuracy on imbalanced data** â†’ 99% "accuracy" predicting all negative  
âŒ **Not using cross-validation** â†’ Overfitting goes undetected  
âŒ **Data leakage in split** â†’ Unrealistic performance estimates  
âŒ **Wrong metric for problem** â†’ Optimizing the wrong objective  
âŒ **Ignoring business context** â†’ Statistically good but business-bad model  

---

<div align="center">

**Measure Right, Build Right** âš–ï¸

*9 topics â€¢ Classification + Regression + Validation*

[â¬…ï¸ Unsupervised ML](../04_unsupervised_ml/) â€¢ [ğŸ  Home](../../README.md) â€¢ [â¡ï¸ Feature Engineering](../06_feature_engineering/)

</div>
